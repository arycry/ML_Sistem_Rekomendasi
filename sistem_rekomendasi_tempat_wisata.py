# -*- coding: utf-8 -*-
"""Sistem rekomendasi Tempat Wisata.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1NAAtFslEsrtczMxwypsSpfrZxR9I07Ei

# Proyek Machine Learning Terapan: Sistem Rekomendasi Tempat Wisata
- **Nama:** Dita Ary Crystian
- **Email:** arycrystian01@gmail.com
- **ID Dicoding:** dita_ary_crystian

# Data Understanding

Pada langkah pertama, lakukan import library yang diperlukan untuk pengolahan dataset nantinya.
"""

#import library
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from zipfile import ZipFile
import tensorflow as tf
from tensorflow import keras
from tensorflow.keras import layers
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.metrics.pairwise import cosine_similarity
from pathlib import Path


import warnings
warnings.simplefilter(action='ignore', category=FutureWarning)

"""Pada langkah ini,  upload kaggle.json yang berisi username dan key dari akun kaggle pengguna."""

# Import module yang disediakan google colab untuk kebutuhan upload file

from google.colab import files
files.upload()

"""Setelah upload kaggle.json, download dataset dari kaggle, pada kasus ini yaitu dataset destinasi tempat wisata di indonesia, lalu ekstrak dataset itu, lalu  deskripsikan variabel untuk masing-masing data csv yang ada di dataset ini."""

# Buat folder .kaggle dan pindahkan kaggle.json ke sana
!mkdir -p /root/.kaggle
!cp kaggle.json /root/.kaggle/
!chmod 600 /root/.kaggle/kaggle.json

# Cek sudah benar
!ls -l /root/.kaggle

#Download dataset
!kaggle datasets download -d aprabowo/indonesia-tourism-destination

#ekstrak dataset yang telah didownload
!unzip indonesia-tourism-destination.zip

#definisi variabel untuk masing-masing dataset
package_df = pd.read_csv('package_tourism.csv')
rating_df = pd.read_csv('tourism_rating.csv')
location_df = pd.read_csv('tourism_with_id.csv')
user_df = pd.read_csv('user.csv')

print('Banyaknya data package: ', len(package_df))
print('Banyaknya data rating: ', len(rating_df))
print('Banyaknya data location: ', len(location_df))
print('Banyaknya data user: ', len(user_df))

"""Terlihat bahwa dataset indonesia tourism destination memiliki 4 sub-dataset, yaitu untuk package, rating, location, serta user, dengan masing-masing banyaknya data yaitu 100 untuk package, 10000 untuk rating, 437 untuk location, dan 300 untuk user.

## Univariate EDA

Pada langkah ini, lakukan pengecekan banyaknya data dari masing-masing variabel serta beberapa nilai unik yang akan diidentifikasi.

### Dataset package
"""

package_df.head()

print('Banyak data: ', len(package_df['Package'].unique()))
print('Banyak Kota: ', package_df['City'].unique())

package_df.info()

feature = 'City'
count = package_df[feature].value_counts()
percent = 100*package_df[feature].value_counts(normalize=True)

df_vis = pd.DataFrame({'jumlah sampel':count, 'persentase':percent.round(1)})
print(df_vis)
count.plot(kind='bar', title=feature);

"""Berikut merupakan penjelasan tiap variabel di dataset package:
- Package: ID unik untuk setiap paket wisata.
- City: Kota di mana tempat wisata berada.
- Place_Tourism1 : Destinasi wisata pertama yang termasuk dalam paket
- Place_Tourism2 : Destinasi wisata kedua yang termasuk dalam paket
- Place_Tourism3 : Destinasi wisata ketiga yang termasuk dalam paket
- Place_Tourism4 : Destinasi wisata keempat yang termasuk dalam paket
- Place_Tourism5 : Destinasi wisata kelima yang termasuk dalam paket

Terlihat bahwa dataset package memiliki 100 data dan 7 variabel, dengan ada nya nilai null pada variabel Place_tourism4 dan Place_tourism5. Selain itu, pada dataset ini memiliki 5 data unik untuk variabel City, yaitu Jakarta, Yogyakarta, Bandung, Semarang, Surabaya, dengan banyaknya data di tiap kota adalah 20 paket.

## Dataset Rating
"""

rating_df.head()

print('Banyak user: ', len(rating_df['User_Id'].unique()))
print('Banyak rating: ', (rating_df['Place_Ratings'].unique()))

rating_df.info()

feature = 'Place_Ratings'
count = rating_df[feature].value_counts()
percent = 100*rating_df[feature].value_counts(normalize=True)

df_vis = pd.DataFrame({'jumlah sampel':count, 'persentase':percent.round(1)})
print(df_vis)
count.plot(kind='bar', title=feature);

"""Berikut merupakan penjelasan tiap variabel di dataset rating:
- User_Id        : ID unik untuk setiap user.
- Place_Id       : ID unik untuk setiap tempat wisata.
- Place_Ratings  : Rating untuk setiap tempat wisata dari user.

Terlihat bahwa dataset rating memiliki 10000 data dan 3 variabel. Pada dataset ini juga memiliki 300 nilai unik pada variabel User_Id dan 5 nilai unik pada Place_Ratings, dengan nilai 1,2,3,4,5, dengan rating 4 memiliki data paling banyak daripada rating lainnya dengan selisih yang tipis.

## Dataframe location
"""

location_df.head()

print('Banyak tempat wisata: ', len(location_df['Place_Name'].unique()))
print('Tipe Kategori: ', location_df['Category'].unique())

location_df.info()

feature = 'Category'
count = location_df[feature].value_counts()
percent = 100*location_df[feature].value_counts(normalize=True)

df_vis = pd.DataFrame({'jumlah sampel':count, 'persentase':percent.round(1)})
print(df_vis)
count.plot(kind='bar', title=feature);

"""Berikut merupakan penjelasan tiap variabel di dataset location:
- Place_Id: ID unik untuk setiap tempat wisata.
- Place_Name: Nama tempat wisata.
- Description: Deskripsi singkat mengenai tempat wisata.
- Category: Kategori tempat wisata (misalnya, Budaya, Taman Hiburan, Cagar Alam).
- City: Kota di mana tempat wisata berada.
- Price: Harga tiket masuk ke tempat wisata (dalam rupiah).
- Rating: Penilaian rata-rata dari pengunjung (skala 1-5).
- Time_Minutes: Waktu yang diperlukan untuk mengunjungi tempat wisata (dalam menit).
- Coordinate: Koordinat geografis tempat wisata.
- Lat: Garis lintang lokasi tempat wisata.
- Long: Garis bujur lokasi tempat wisata.
- Unnamed: 11 : Tidak diketahui.
- Unnamed: 12 : Tidak diketahui.

Terlihat bahwa dataset location memiliki 437 data dan 13 variabel, dimana merupakan banyak nya data unik dari tempat wisata , dan adanya nilai null pada kolom Time_Minutes dan Unnamed: 11. Pada dataset ini memiliki 6 nilai unik pada kolom Category, yaitu Budaya, Taman Hiburan, Cagar Alam, Bahari, Pusat Perbelanjaan, Tempat Ibadah dan Taman hiburan memiliki data paling banyak dibandingkan kategori lain dengan 135 data.

## Dataset user
"""

user_df.head()

print('Banyak user: ', len(user_df['User_Id'].unique()))
print('Lokasi user: ', len(user_df['Location'].unique()))

user_df.info()

user_df.describe()

feature = 'Location'
count = user_df[feature].value_counts()
percent = 100*user_df[feature].value_counts(normalize=True)

top_5_count = count.head(5)
top_5_percent = percent.head(5)

df_vis = pd.DataFrame({'jumlah sampel':top_5_count, 'persentase':top_5_percent.round(1)})
print(df_vis)
top_5_count.plot(kind='bar', title=feature);

"""Berikut merupakan penjelasan tiap variabel di dataset user:
- User_Id: ID unik untuk setiap pengguna.
- Location: Kota atau lokasi asal pengguna.
- Age: Usia pengguna.

Terlihat bahwa dataset user memiliki 300 data dan 3 variabel, yang merupakan banyaknya nilai unik pada user. Pada dataset ini memiliki 28 nilai unik untuk asal daerah user, yaitu kolom Location. Selain itu dataset memiliki data di kolom Age, yaitu usia user, dengan rentang 24 sampai dengan 40 tahun. Serta 5 user pemberi rating terbanyak berada di Bekasi, Semarang, Yogyakarta, Lampung, dan Bogor.

#Data Preparation

##Preprocessing untuk EDA

### Mengecek total user dan location

Pada langkah ini, bagi 4 dataset menjadi 2 bagian, yaitu user dan location(tempat wisata). Setelah itu, gabungkan dataset rating_df dan user_df untuk mengetahui total dari user, serta gabungkan dataset location_df dan rating_df untuk mengetahui total dari location.

Pada langkah ini, gabungkan dataset dari user untuk mengetahui jumlah seluruh user yang ada.
"""

# Menggabungkan seluruh userID
user_all = np.concatenate((
    rating_df['User_Id'].unique(),
    user_df['User_Id'].unique()
))

# Menghapus data yang sama kemudian mengurutkannya
user_all = np.sort(np.unique(user_all))

print('Jumlah seluruh user: ', len(user_all))

"""Terlihat bahwa jumlah seluruh user yaitu 300.

Setelah itu, gabungkan dataset dari location untuk mengetahui jumlah location  yang ada.
"""

# Menggabungkan seluruh Place
place_all = np.concatenate((
    location_df['Place_Id'].unique(),
    rating_df['Place_Id'].unique()
))

# Menghapus data yang sama kemudian mengurutkannya
place_all = np.sort(np.unique(place_all))

print('Jumlah seluruh lokasi: ', len(place_all))

"""Terlihat bahwa jumlah location yaitu 437 data.

### Mengecek jumlah rating

Selanjutnya, buat variabel baru, yaitu place, dengan menggabungkan data rating dan place untuk melihat banyaknya rating dari gabungan data yang ada, pada kasus ini data rating_df dan location_df
"""

place = pd.merge(rating_df, location_df , on='Place_Id', how='left')
place

place.info()

place.isnull().sum()

"""Terlihat bahwa ada 10000 data rating yang berasal dari gabungan dataset rating_df dan location_df, serta data ini memiliki nilai null pada kolom time_minutes dan Unnamed: 11.

Setelah itu, ambil fitur numerik terlebih dahulu, lalu cari jumlah dari fitur numerik tersebut berdasarkan Place-Id.
"""

# Pilih hanya kolom numerik
numerical_data = place.select_dtypes(include='number')

# Groupby 'Place-ID' dan hitung jumlahnya
result = numerical_data.groupby(place['Place_Id']).sum()
print(result)

"""## Data Preprocessing untuk *Content Based Filtering*

###Membuat dataframe untuk Modeling Content Based Filtering

Setelah itu, buat variabel baru, yaitu  all_place_name yang akan memuat data rating_df dan akan digabungkan dengan kolom Place_Id, Place_Name,
Category, City dari dataset location_df berdasarkan Place-Id.
"""

all_place_name = rating_df
all_place_name

# Menggabungkan all resto_rate dengan dataframe geo berdasarkan placeID
all_place_name = pd.merge(all_place_name, location_df[['Place_Id','Place_Name', 'Category', 'City']], on='Place_Id', how='left')

# Print dataframe all_resto_name
all_place_name

"""###Mengecek nilai null

Setelah itu, cek nilai null, deskripsi analisis untuk fitur numerik, dan melihat banyaknya data unik dari kolom Place_Id, Place_Name, dan Category
"""

all_place_name.isnull().sum()

"""###Mengecek data duplikat

Setelah itu, cek nilai duplikat dari data tersebut.
"""

duplicate_rows = all_place_name.duplicated()

print(duplicate_rows.sum())

"""Karena ada duplikat data, hapus data duplikat tersebut"""

all_place_name = all_place_name.drop_duplicates()

"""###Mengecek deskripsi analisis dan banyaknya data unik

Pada tahap ini, kita akan mengecek deskripsi analisis dan kita akan mengecek banyaknya data unik pada kolom Place_Id, Place_Name, dan Location.
"""

all_place_name.describe()

len(all_place_name['Place_Id'].unique())

len(all_place_name['Place_Name'].unique())

len(all_place_name['Category'].unique())

"""Bisa terlihat bahwa data sudah bersih dari nilai null, dan data memiliki 437 nilai unik pada Place_Id, 437 nilai unik pada Place_Name, dan 6 data unik pada Category, Selain itu, data memiliki 300 user dan rentang rating dari 1,2,3,4,5.

###mengecek kembali data untuk Content Based Filtering

Pada langkah ini, cek kembali data yang telah di preprocessing di langkah sebelumnya. Setelah itu, urutkan data terlebih dahulu berdasarkan Place_Id
"""

all_place_name = all_place_name.sort_values('Place_Id', ascending=True)
all_place_name

"""###mengecek nilai unik dari kolom Category

Setelah itu, cek nilai unik dari kolom Category untuk mengecek apakah ada kategori tempat wisata.
"""

all_place_name['Category'].unique()

"""Terlihat bahwa data mempunyai kategori tempat wisata yaitu Budaya, Taman Hiburan, Cagar Alam, Pusat Perbelanjaan, dan Tempat Ibadah

###Menghapus duplikat pada Place_Id

Setelah itu, buat variabel preparation yang berisi data all_place_name. Lalu urutkan data berdasarkan Place_Id.
"""

preparation = all_place_name
preparation.sort_values('Place_Id')

"""Lalu hapus nilai duplikat dari Place_Id karena  hanya menggunakan data unik dari data yang ada."""

preparation = preparation.drop_duplicates('Place_Id')
preparation

"""###Membuat data dictionary untuk modeling

Setelah itu, konversi data series menjadi list, sehingga gunakan fungsi *tolist* untuk konversi data.
"""

# Mengonversi data series ‘placeID’ menjadi dalam bentuk list
place_id = preparation['Place_Id'].tolist()

# Mengonversi data series ‘Name’ menjadi dalam bentuk list
place_name = preparation['Place_Name'].tolist()

# Mengonversi data series ‘Rcuisine’ menjadi dalam bentuk list
place_cat = preparation['Category'].tolist()

print(len(place_id))
print(len(place_name))
print(len(place_cat))

"""Setelah itu, buat data dictionary untuk data place_id, place_name, dan place_cat(place_category). Lalu cek data yang telah dibuat."""

place_new = pd.DataFrame({
    'id': place_id,
    'name': place_name,
    'category': place_cat
})

place_new

"""### Ekstraksi TF-IDF

Pada langkah ini, buat variabel data yang berisi data place_new.
"""

data = place_new
data.head()

"""Setelah itu, panggil fungsi TfidfVectorizer lalu  fit dengan kolom Category."""

# Inisialisasi TfidfVectorizer
tf = TfidfVectorizer()

# Melakukan perhitungan idf pada data cuisine
tf.fit(data['category'])

# Mapping array dari fitur index integer ke fitur nama
tf.get_feature_names_out()

"""Kemudian lakukan fit transform dan melihat ukuran matrix setelah dilakukan fit transform TF-IDF"""

tfidf_matrix = tf.fit_transform(data['category'])

# Melihat ukuran matrix tfidf
tfidf_matrix.shape

"""Kemudian lihat data dalam bentuk matrix, namun sebelum itu, gunakan fungsi todense untuk menghasilkan vektor TF-IDF dalam bentuk matrix."""

# Mengubah vektor tf-idf dalam bentuk matriks dengan fungsi todense()
tfidf_matrix.todense()

"""Setelah itu, lihat hasil dari TF-IDF dibawah ini."""

pd.DataFrame(
    tfidf_matrix.todense(),
    columns=tf.get_feature_names_out(),
    index=data.name
).sample(10, axis=1).sample(10, axis=0)

"""Bisa terlihat bahwa Museum Perangko memiliki nilai 1 pada kolom budaya, sehingga Museum Perangko termasuk ke dalam kategori budaya, dan sebagainya.

##Data Preprocessing untuk Collaborative Filtering

###Membuat dataframe untuk Modeling Collaborative Filtering

Pada bagian ini, langkah pertama yaitu buat variabel df yang memuat dataset rating_df.
"""

df = rating_df
df

"""Setelah itu, lakukan persiapan data untuk menyandikan (encode) fitur User_Id ke dalam indeks integer."""

# Mengubah userID menjadi list tanpa nilai yang sama
user_ids = df['User_Id'].unique().tolist()
print('list userID: ', user_ids)

# Melakukan encoding userID
user_to_user_encoded = {x: i for i, x in enumerate(user_ids)}
print('encoded userID : ', user_to_user_encoded)

# Melakukan proses encoding angka ke ke userID
user_encoded_to_user = {i: x for i, x in enumerate(user_ids)}
print('encoded angka ke userID: ', user_encoded_to_user)

"""Setelah itu, lakukan persiapan data untuk menyandikan (encode) fitur Place_Id ke dalam indeks integer."""

# Mengubah placeID menjadi list tanpa nilai yang sama
place_ids = df['Place_Id'].unique().tolist()

# Melakukan proses encoding placeID
place_to_place_encoded = {x: i for i, x in enumerate(place_ids)}

# Melakukan proses encoding angka ke placeID
place_encoded_to_place = {i: x for i, x in enumerate(place_ids)}

"""Berikutnya, petakan userID dan placeID ke dataframe yang berkaitan."""

# Mapping userID ke dataframe user
df['user'] = df['User_Id'].map(user_to_user_encoded)

# Mapping placeID ke dataframe place
df['place'] = df['Place_Id'].map(place_to_place_encoded)

"""Setelah itu, cek beberapa hal dalam data seperti jumlah user, jumlah tempat wisata, dan mengubah nilai rating menjadi float."""

# Mendapatkan jumlah user
num_users = len(user_to_user_encoded)
print(num_users)

# Mendapatkan jumlah place
num_place = len(place_encoded_to_place)
print(num_place)

# Mengubah rating menjadi nilai float
df['Place_Ratings'] = df['Place_Ratings'].values.astype(np.float32)

# Nilai minimum rating
min_rating = min(df['Place_Ratings'])

# Nilai maksimal rating
max_rating = max(df['Place_Ratings'])

print('Number of User: {}, Number of place: {}, Min Rating: {}, Max Rating: {}'.format(
    num_users, num_place, min_rating, max_rating
))

"""Terlihat bahwa dataframe memiliki data user sebanyak 300 data, data place sebanyak 437 data, serta data rating dari 1.0 sampai 5.0.

###Acak data

Setelah itu, acak datanya terlebih dahulu agar distribusinya menjadi random.
"""

df = df.sample(frac=1, random_state=42)
df

"""Karena pada kolom user tidak terjadi pengacakan, maka acak kolom user terlebih dahulu."""

df['user'] = np.random.permutation(df['user'].values)
df

"""###Membagi data train dan data validation

Selanjutnya, bagi data train dan validasi dengan komposisi 80:20 dari data yang sudah disiapkan di proses data preparation. Namun sebelumnya, petakan (mapping) data user dan place menjadi satu value terlebih dahulu. Lalu, buatlah rating dalam skala 0 sampai 1 agar mudah dalam melakukan proses training.
"""

# Membuat variabel x untuk mencocokkan data user dan resto menjadi satu value
x = df[['user', 'place']].values

# Membuat variabel y untuk membuat rating dari hasil
y = df['Place_Ratings'].apply(lambda x: (x - min_rating) / (max_rating - min_rating)).values

# Membagi menjadi 80% data train dan 20% data validasi
train_indices = int(0.8 * df.shape[0])
x_train, x_val, y_train, y_val = (
    x[:train_indices],
    x[train_indices:],
    y[:train_indices],
    y[train_indices:]
)

print(x, y)

"""# Modeling

## Content Based Filtering

### Cosine Similarity

Selanjutnya, hitung derajat kesamaan (similarity degree) antar tempat wisata dengan teknik cosine similarity
"""

# Menghitung cosine similarity pada matrix tf-idf
cosine_sim = cosine_similarity(tfidf_matrix)
cosine_sim

"""Selanjutnya, lihat matriks kesamaan setiap tempat wisata dengan menampilkan nama tempat wisata dalam 5 sampel kolom (axis = 1) dan 10 sampel baris (axis=0)."""

cosine_sim_df = pd.DataFrame(cosine_sim, index=data['name'], columns=data['name'])
print('Shape:', cosine_sim_df.shape)

# Melihat similarity matrix pada setiap lokasi
cosine_sim_df.sample(5, axis=1).sample(10, axis=0)

"""Pada contohnya, bisa terlihat bahwa Curug Cipanas memiliki angka 1 dengan Bumi Perkemahan Batu Kuda dan Umbul Sidomukti. Ini berarti bahwa Curug Cipanas memiliki kategori yang sama dengan Bumi Perkemahan Batu Kuda dan Umbul Sidomukti.

### Modeling Content Based Filtering

Setelah itu, buat fungsi untuk mendapatkan rekomendasi tempat wisata dengan data yang suda kita siapkan sebelumnya di proses data preparation. Pada fungsi ini, buat fungsi untuk mendapatkan 5 rekomendasi tempat wisata dengan nama yang diinputkan nanti.
"""

def location_recommendations(nama, similarity_data=cosine_sim_df, items=data[['name', 'category']], k=5):
    """
    Rekomendasi Resto berdasarkan kemiripan dataframe

    Parameter:
    ---
    nama : tipe data string (str)
                Nama Tempat Wisata (index kemiripan dataframe)
    similarity_data : tipe data pd.DataFrame (object)
                      Kesamaan dataframe, simetrik, dengan resto sebagai
                      indeks dan kolom
    items : tipe data pd.DataFrame (object)
            Mengandung kedua nama dan fitur lainnya yang digunakan untuk mendefinisikan kemiripan
    k : tipe data integer (int)
        Banyaknya jumlah rekomendasi yang diberikan
    ---


    Pada index ini, kita mengambil k dengan nilai similarity terbesar
    pada index matrix yang diberikan (i).
    """


    # Mengambil data dengan menggunakan argpartition untuk melakukan partisi secara tidak langsung sepanjang sumbu yang diberikan
    # Dataframe diubah menjadi numpy
    # Range(start, stop, step)
    index = similarity_data.loc[:,nama].to_numpy().argpartition(
        range(-1, -k, -1))

    # Mengambil data dengan similarity terbesar dari index yang ada
    closest = similarity_data.columns[index[-1:-(k+2):-1]]

    # Drop nama_resto agar nama resto yang dicari tidak muncul dalam daftar rekomendasi
    closest = closest.drop(nama, errors='ignore')

    return pd.DataFrame(closest).merge(items).head(k)

"""Setelah itu, masukkan data yang akan dicari rekomendasi nya."""

data[data.name.eq('Kebun Binatang Ragunan')]

"""Kemudian, cari rekomendasi nya dengan fungsi yang telah kita buat sebelumnya."""

location_recommendations('Kebun Binatang Ragunan')

"""Terlihat bahwa fungsi merekomendasikan tempat wisata dengan category yang sama.

## Model Development dengan Collaborative Filtering

### Collaborative Filtering Modeling
"""

# Import library
import pandas as pd
import numpy as np
from zipfile import ZipFile
import tensorflow as tf
from tensorflow import keras
from tensorflow.keras import layers
from pathlib import Path
import matplotlib.pyplot as plt

"""Setelah itu, buat class RecommenderNet dengan keras Model class."""

class RecommenderNet(tf.keras.Model):

  # Insialisasi fungsi
  def __init__(self, num_users, num_place, embedding_size, **kwargs):
    super(RecommenderNet, self).__init__(**kwargs)
    self.num_users = num_users
    self.num_place = num_place
    self.embedding_size = embedding_size
    self.user_embedding = layers.Embedding( # layer embedding user
        num_users,
        embedding_size,
        embeddings_initializer = 'he_normal',
        embeddings_regularizer = keras.regularizers.l2(1e-4)
    )
    self.user_bias = layers.Embedding(num_users, 1) # layer embedding user bias
    self.place_embedding = layers.Embedding( # layer embeddings place
        num_place,
        embedding_size,
        embeddings_initializer = 'he_normal',
        embeddings_regularizer = keras.regularizers.l2(1e-6)
    )
    self.place_bias = layers.Embedding(num_place, 1) # layer embedding place bias

    self.dropout = layers.Dropout(0.3)

  def call(self, inputs):
    user_vector = self.user_embedding(inputs[:,0]) # memanggil layer embedding 1
    user_vector = self.dropout(user_vector)
    user_bias = self.user_bias(inputs[:, 0]) # memanggil layer embedding 2
    place_vector = self.place_embedding(inputs[:, 1]) # memanggil layer embedding 3
    place_vector = self.dropout(place_vector)
    place_bias = self.place_bias(inputs[:, 1]) # memanggil layer embedding 4

    dot_user_place = tf.tensordot(user_vector, place_vector, 2)

    x = dot_user_place + user_bias + place_bias

    return tf.nn.sigmoid(x) # activation sigmoid

"""Setelah itu, lakukan proses compile terhadap model, dengan menggunakan fungsi loss binary crossentropy, optimizer adam, dan matriks RMSE."""

model = RecommenderNet(num_users, num_place, 20) # inisialisasi model

# model compile
model.compile(
    loss = tf.keras.losses.BinaryCrossentropy(),
    optimizer = keras.optimizers.Adam(learning_rate=0.001),
    metrics=[tf.keras.metrics.RootMeanSquaredError()]
)

"""Setelah itu, lakukan training data."""

# Memulai training

history = model.fit(
    x = x_train,
    y = y_train,
    batch_size = 8,
    epochs = 100,
    validation_data = (x_val, y_val)
)

"""Setelah itu, lakukan visualisasi proses training, plot metrik evaluasi dengan matplotlib."""

plt.plot(history.history['root_mean_squared_error'])
plt.plot(history.history['val_root_mean_squared_error'])
plt.title('model_metrics')
plt.ylabel('root_mean_squared_error')
plt.xlabel('epoch')
plt.legend(['train', 'test'], loc='upper left')
plt.show()

"""Sebelum melakukan prediksi sistem rekomendasi, buat variabel place_not_visited terlebih dahulu  karena daftar place_not_visited inilah yang akan menjadi resto yang direkomendasikan."""

place_df = place_new
df = pd.read_csv('tourism_rating.csv')

# Mengambil sample user
user_id = df['User_Id'].sample(1).iloc[0]
place_visited_by_user = df[df['User_Id'] == user_id]

# Operator bitwise (~), bisa diketahui di sini https://docs.python.org/3/reference/expressions.html
place_not_visited = place_df[~place_df['id'].isin(place_visited_by_user['Place_Id'].values)]['id']
place_not_visited = list(
    set(place_not_visited)
    .intersection(set(place_to_place_encoded.keys()))
)

place_not_visited = [[place_to_place_encoded.get(x)] for x in place_not_visited]
user_encoder = user_to_user_encoded.get(user_id)
user_place_array = np.hstack(
    ([[user_encoder]] * len(place_not_visited), place_not_visited)
)

"""Selanjutnya, lakukan prediksi untuk memperoleh 10 rekomendasi tempat wisata."""

ratings = model.predict(user_place_array).flatten()

top_ratings_indices = ratings.argsort()[-10:][::-1]
recommended_place_ids = [
    place_encoded_to_place.get(place_not_visited[x][0]) for x in top_ratings_indices
]

print('Showing recommendations for users: {}'.format(user_id))
print('===' * 9)
print('place with high ratings from user')
print('----' * 8)

top_place_user = (
    place_visited_by_user.sort_values(
        by = 'Place_Ratings',
        ascending=False
    )
    .head(5)['Place_Ratings'].values
)

place_df_rows = place_df[place_df['id'].isin(top_place_user)]
for row in place_df_rows.itertuples():
    print(row.name, ':', row.category)

print('----' * 8)
print('Top 10 place recommendation')
print('----' * 8)

recommended_place = place_df[place_df['id'].isin(recommended_place_ids)]
for row in recommended_place.itertuples():
    print(row.name, ':', row.category)
