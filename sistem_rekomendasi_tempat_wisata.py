# -*- coding: utf-8 -*-
"""Sistem rekomendasi Tempat Wisata.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1NAAtFslEsrtczMxwypsSpfrZxR9I07Ei

# Proyek Machine Learning Terapan: Sistem Rekomendasi Tempat Wisata
- **Nama:** Dita Ary Crystian
- **Email:** arycrystian01@gmail.com
- **ID Dicoding:** dita_ary_crystian

# Data Understanding

Pada langkah pertama, kita akan melakukan import library yang diprlukan untuk pengolahan data kita nantinya.
"""

#import library
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from zipfile import ZipFile
import tensorflow as tf
from tensorflow import keras
from tensorflow.keras import layers
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.metrics.pairwise import cosine_similarity
from pathlib import Path


import warnings
warnings.simplefilter(action='ignore', category=FutureWarning)

"""Pada langkah ini, kita perlu upload kaggle.json yang berisi username dan key dari akun kaggle kita."""

# Import module yang disediakan google colab untuk kebutuhan upload file

from google.colab import files
files.upload()

"""Setelah uplaod kaggle.json, kita akan download dataset dari kaggle, pada kasus ini yaitu dataset destinasi tempat wisata di indonesia, lalu kita ekstrak dataset itu, lalu kita akan deskripsikan variabel untuk masing-masing data csv yang ada di dataset ini."""

#Download dataset
!kaggle datasets download -d aprabowo/indonesia-tourism-destination

#ekstrak dataset yang telah didownload
!unzip indonesia-tourism-destination.zip

#definisi variabel untuk masing-masing dataset
package_df = pd.read_csv('package_tourism.csv')
rating_df = pd.read_csv('tourism_rating.csv')
location_df = pd.read_csv('tourism_with_id.csv')
user_df = pd.read_csv('user.csv')

print('Banyaknya data package: ', len(package_df))
print('Banyaknya data rating: ', len(rating_df))
print('Banyaknya data location: ', len(location_df))
print('Banyaknya data user: ', len(user_df))

"""Terlihat bahwa kita memiliki 4 dataset, yaitu untuk package, rating, location, serta user, dengan masing-masing banyaknya data yaitu 100 untuk package, 1000 untuk rating, 437 untuk location, dan 300 untuk user.

## Univariate EDA

Pada langkah ini, kita akan mengecek banyaknya data dari masing-masing variabel serta beberapa nilai unik yang akan diidentifikasi.

## Dataset package
"""

package_df.head()

print('Banyak data: ', len(package_df['Package'].unique()))
print('Banyak Kota: ', package_df['City'].unique())

package_df.info()

feature = 'City'
count = package_df[feature].value_counts()
percent = 100*package_df[feature].value_counts(normalize=True)

df_vis = pd.DataFrame({'jumlah sampel':count, 'persentase':percent.round(1)})
print(df_vis)
count.plot(kind='bar', title=feature);

"""Berikut merupakan penjelasan tiap variabel di dataset package:
- Package: ID unik untuk setiap paket wisata.
- City: Kota di mana tempat wisata berada.
- Place_Tourism1 : Destinasi wisata pertama yang termasuk dalam paket
- Place_Tourism2 : Destinasi wisata kedua yang termasuk dalam paket
- Place_Tourism3 : Destinasi wisata ketiga yang termasuk dalam paket
- Place_Tourism4 : Destinasi wisata keempat yang termasuk dalam paket
- Place_Tourism5 : Destinasi wisata kelima yang termasuk dalam paket

Terlihat bahwa kita memiliki 100 data dan 7 variabel, dengan ada nya nilai null pada variabel Place_tourism4 dan Place_tourism5. Selain itu, pada dataset ini memiliki 5 data unik untuk variabel City, yaitu Jakarta, Yogyakarta, Bandung, Semarang, Surabaya, dengan banyaknya data di tiap kota adalah 20 paket.

## Dataset Rating
"""

rating_df.head()

print('Banyak user: ', len(rating_df['User_Id'].unique()))
print('Banyak rating: ', (rating_df['Place_Ratings'].unique()))

rating_df.info()

feature = 'Place_Ratings'
count = rating_df[feature].value_counts()
percent = 100*rating_df[feature].value_counts(normalize=True)

df_vis = pd.DataFrame({'jumlah sampel':count, 'persentase':percent.round(1)})
print(df_vis)
count.plot(kind='bar', title=feature);

"""Berikut merupakan penjelasan tiap variabel di dataset rating:
- User_Id        : ID unik untuk setiap user.
- Place_Id       : ID unik untuk setiap tempat wisata.
- Place_Ratings  : Rating untuk setiap tempat wisata dari user.

Terlihat bahwa kita memiliki 10000 data dan 3 variabel. Pada dataset ini,  kita juga memiliki 300 nilai unik pada variabel User_Id dan 5 nilai unik pada Place_Ratings, dengan nilai 1,2,3,4,5, dengan rating 4 memiliki data paling banyak daripada rating lainnya dengan selisih yang tipis.

## Dataframe location
"""

location_df.head()

print('Banyak tempat wisata: ', len(location_df['Place_Name'].unique()))
print('Tipe Kategori: ', location_df['Category'].unique())

location_df.info()

feature = 'Category'
count = location_df[feature].value_counts()
percent = 100*location_df[feature].value_counts(normalize=True)

df_vis = pd.DataFrame({'jumlah sampel':count, 'persentase':percent.round(1)})
print(df_vis)
count.plot(kind='bar', title=feature);

"""Berikut merupakan penjelasan tiap variabel di dataset location:
- Place_Id: ID unik untuk setiap tempat wisata.
- Place_Name: Nama tempat wisata.
- Description: Deskripsi singkat mengenai tempat wisata.
- Category: Kategori tempat wisata (misalnya, Budaya, Taman Hiburan, Cagar Alam).
- City: Kota di mana tempat wisata berada.
- Price: Harga tiket masuk ke tempat wisata (dalam rupiah).
- Rating: Penilaian rata-rata dari pengunjung (skala 1-5).
- Time_Minutes: Waktu yang diperlukan untuk mengunjungi tempat wisata (dalam menit).
- Coordinate: Koordinat geografis tempat wisata.
- Lat: Garis lintang lokasi tempat wisata.
- Long: Garis bujur lokasi tempat wisata.
- Unnamed: 11 : Tidak diketahui.
- Unnamed: 12 : Tidak diketahui.

Terlihat bahwa kita memiliki 437 data dan 13 variabel, dimana merupakan banyak nya data unik dari tempat wisata yang kita punya, dan adamnya nilai null pada kolom Time_Minutes dan Unnamed: 11. Pada dataset ini, kita memiliki 6 nilai unik pada kolom Category, yaitu Budaya, Taman Hiburan, Cagar Alam, Bahari, Pusat Perbelanjaan, Tempat Ibadah dan Taman hiburan memiliki data paling banyak dibandingkan kategori lain dengan 135 data.

## Dataset user
"""

user_df.head()

print('Banyak user: ', len(user_df['User_Id'].unique()))
print('Lokasi user: ', len(user_df['Location'].unique()))

user_df.info()

user_df.describe()

feature = 'Location'
count = user_df[feature].value_counts()
percent = 100*user_df[feature].value_counts(normalize=True)

top_5_count = count.head(5)
top_5_percent = percent.head(5)

df_vis = pd.DataFrame({'jumlah sampel':top_5_count, 'persentase':top_5_percent.round(1)})
print(df_vis)
top_5_count.plot(kind='bar', title=feature);

"""Berikut merupakan penjelasan tiap variabel di dataset user:
- User_Id: ID unik untuk setiap pengguna.
- Location: Kota atau lokasi asal pengguna.
- Age: Usia pengguna.

Terlihat bahwa kita memiliki 300 data dan 3 variabel, yang merupakan banyaknya nilai unik pada user. Pada dataset ini, kita memiliki 28 nilai unik untuk asal daerah user, yaitu kolom Location. Selain itu, kita memiliki data di kolom Age, yaitu usia user, dengan rentang 24 sampai dengan 40 tahun. Serta 5 user pemberi rating terbanyak berada di Bekasi, Semarang, Yogyakarta, Lampung, dan Bogor.

## Mengecek total user dan location

Kita akan membagi 4 dataset menjadi 2 bagian, yaitu user dan location(tempat wisata). Kita akan menggabungkan dataset rating_df dan user_df untuk mengetahui total dari user, serta kita akan menggabungkan dataset location_df dan rating_df untuk mengetahui total dari location.

Pada langkah ini, kita akan menggabungkan dataset dari user untuk mengetahui jumlah seluruh user yang ada.
"""

# Menggabungkan seluruh userID
user_all = np.concatenate((
    rating_df['User_Id'].unique(),
    user_df['User_Id'].unique()
))

# Menghapus data yang sama kemudian mengurutkannya
user_all = np.sort(np.unique(user_all))

print('Jumlah seluruh user: ', len(user_all))

"""Terlihat bahwa jumlah seluruh user yaitu 300.

Setelah itu, kita akan menggabungkan dataset dari location untuk mengetahui jumlah location  yang ada.
"""

# Menggabungkan seluruh Place
place_all = np.concatenate((
    location_df['Place_Id'].unique(),
    rating_df['Place_Id'].unique()
))

# Menghapus data yang sama kemudian mengurutkannya
place_all = np.sort(np.unique(place_all))

print('Jumlah seluruh lokasi: ', len(place_all))

"""Terlihat bahwa jumlah location yaitu 437 data.

## Mengecek jumlah rating

Selanjutnya, kita akan membuat variabel baru, yaitu place, dengan menggabungkan data rating dan place untuk melihat banyaknya rating dari gabungan data yang ada, pada kasus ini data rating_df dan location_df
"""

place = pd.merge(rating_df, location_df , on='Place_Id', how='left')
place

place.info()

place.isnull().sum()

"""Terlihat bahwa kita memiliki 10000 data rating dari gabungan dataset rating_df dan location_df, serta data ini memiliki nilai null pada kolom time_minutes dan Unnamed: 11.

Setelah itu, kita akan mengambil fitur numerik terlebih dahulu, lalu kita akan mencari jumlah dari fitur numerik tersebut berdasarkan Place-Id.
"""

# Pilih hanya kolom numerik
numerical_data = place.select_dtypes(include='number')

# Groupby 'Place-ID' dan hitung jumlahnya
result = numerical_data.groupby(place['Place_Id']).sum()
print(result)

"""## Data Preprocessing

Setelah itu, kita akan membuat variabel baru, yaitu  all_place_name yang akan memuat data rating_df dan akan digabungkan dengan kolom Place_Id, Place_Name,
Category, City dari dataset location_df berdasarkan Place-Id.
"""

all_place_name = rating_df
all_place_name

# Menggabungkan all resto_rate dengan dataframe geo berdasarkan placeID
all_place_name = pd.merge(all_place_name, location_df[['Place_Id','Place_Name', 'Category', 'City']], on='Place_Id', how='left')

# Print dataframe all_resto_name
all_place_name

"""Setelah itu, kita akan mengecek nilai null, deskripsi analisis untuk fitur numerik, dan melihat banyaknya data unik dari kolom Place_Id, Place_Name, dan Category"""

all_place_name.isnull().sum()

"""Setelah itu, kita akan mengecek nilai duplikat dari data tersebut."""

duplicate_rows = all_place_name.duplicated()

print(duplicate_rows.sum())

"""Karena ada duplikat data, kita perlu menghapus data duplikat tersebut"""

all_place_name = all_place_name.drop_duplicates()

all_place_name.describe()

len(all_place_name['Place_Id'].unique())

len(all_place_name['Place_Name'].unique())

len(all_place_name['Category'].unique())

"""Bisa terlihat bahwa kita bersih dari nilai null, dan data kita memiliki 437 nilai unik pada Place_Id, 437 nilai unik pada Place_Name, dan 6 data unik pada Category, Selain itu,  kita memiliki 300 user dan rentang rating dari 1,2,3,4,5.

# Data Preparation

Kita akan mengecek kembali data yang telah kita preprocessing di langkah sebelumnya. Kita akan mengurutkan data terlebih dahulu berdasarkan Place_Id
"""

all_place_name = all_place_name.sort_values('Place_Id', ascending=True)
all_place_name

"""Setelah itu, kita akan mengecek nilai unik dari kolom Category untuk mengecek apakah ada kategori tempat wisata."""

all_place_name['Category'].unique()

"""Terlihat bahwa kita mempunyai kategori tempat wisata yaitu Budaya, Taman Hiburan, Cagar Alam, Pusat Perbelanjaan, dan Tempat Ibadah

Setelah itu, kita akan membuat variabel preparation yang berisi data all_place_name. Lalu kita akan mengurutkan data berdasarkan Place_Id.
"""

preparation = all_place_name
preparation.sort_values('Place_Id')

"""Lalu kita akan menghapus nilai duplikat dari Place_Id karena kita hanya menggunakan data unik dari data yang kita punya."""

preparation = preparation.drop_duplicates('Place_Id')
preparation

"""Setelah itu, kita akan konversi data series menjadi list, sehingga kita menggunakan fungsi *tolist* untuk konversi data."""

# Mengonversi data series ‘placeID’ menjadi dalam bentuk list
place_id = preparation['Place_Id'].tolist()

# Mengonversi data series ‘Name’ menjadi dalam bentuk list
place_name = preparation['Place_Name'].tolist()

# Mengonversi data series ‘Rcuisine’ menjadi dalam bentuk list
place_cat = preparation['Category'].tolist()

print(len(place_id))
print(len(place_name))
print(len(place_cat))

"""Setelah itu, kita akan membuat data dictionary untuk data place_id, place_name, dan place_cat(place_category). Lalu kita akan mengecek data yang telah kita buat."""

place_new = pd.DataFrame({
    'id': place_id,
    'name': place_name,
    'category': place_cat
})

place_new

"""# Modeling

## Model development dengan Content Based Filtering

Pada langkah ini, kita akan membuat variabel data yang berisi data place_new.
"""

data = place_new
data.head()

"""### TF-IDF

Pada tahap ini, kita akan memanggil fungsi TfidfVectorizer lalu kita akan fit dengan kolom Category.
"""

# Inisialisasi TfidfVectorizer
tf = TfidfVectorizer()

# Melakukan perhitungan idf pada data cuisine
tf.fit(data['category'])

# Mapping array dari fitur index integer ke fitur nama
tf.get_feature_names_out()

"""Lalu kita akan melakukan fit transform dan melihat ukuran matrix setelah dilakukan fit transform TF-IDF"""

tfidf_matrix = tf.fit_transform(data['category'])

# Melihat ukuran matrix tfidf
tfidf_matrix.shape

"""Lalu kita akan melihat data dalam bentuk matrix, namun sebelum itu, kita akan menggunakan fungsi todense untuk menghasilkan vektor TF-IDF dalam bentuk matrix."""

# Mengubah vektor tf-idf dalam bentuk matriks dengan fungsi todense()
tfidf_matrix.todense()

"""Setelah itu, kita akan melihat hasil dari TF-IDF dibawah ini."""

pd.DataFrame(
    tfidf_matrix.todense(),
    columns=tf.get_feature_names_out(),
    index=data.name
).sample(10, axis=1).sample(10, axis=0)

"""Bisa terlihat bahwa Museum Taman Prasasti memiliki nilai 1 pada kolom budaya, sehingga Museum Taman Prasasti termasuk ke dalam kategori budaya, dan sebagainya.

### Cosine Similarity

Selanjutnya, kita akan menghitung derajat kesamaan (similarity degree) antar tempat wisata dengan teknik cosine similarity
"""

# Menghitung cosine similarity pada matrix tf-idf
cosine_sim = cosine_similarity(tfidf_matrix)
cosine_sim

"""Selanjutnya, kita akan  melihat matriks kesamaan setiap tempat wisata dengan menampilkan nama tempat wisata dalam 5 sampel kolom (axis = 1) dan 10 sampel baris (axis=0)."""

cosine_sim_df = pd.DataFrame(cosine_sim, index=data['name'], columns=data['name'])
print('Shape:', cosine_sim_df.shape)

# Melihat similarity matrix pada setiap lokasi
cosine_sim_df.sample(5, axis=1).sample(10, axis=0)

"""Pada contohnya, bisa kita lihat bahwa Curug Malela memiliki angka 1 dengan Situ Patenggang dan Taman Hutan Raya Ir. H. Juanda. Ini berarti bahwa Curug Mamela memiliki kategori yang sama dengan Situ Patenggang dan Taman Hutan Raya Ir. H. Juanda.

Setelah itu, kita akan membuat fungsi untuk mendapatkan rekomendasi tempat wisata. Pada fungsi ini, kita akan membuat fungsi untuk mendapatkan 5 rekomendasi tempat wisata dengan nama yang kita inputkan nanti.
"""

def location_recommendations(nama, similarity_data=cosine_sim_df, items=data[['name', 'category']], k=5):
    """
    Rekomendasi Resto berdasarkan kemiripan dataframe

    Parameter:
    ---
    nama : tipe data string (str)
                Nama Tempat Wisata (index kemiripan dataframe)
    similarity_data : tipe data pd.DataFrame (object)
                      Kesamaan dataframe, simetrik, dengan resto sebagai
                      indeks dan kolom
    items : tipe data pd.DataFrame (object)
            Mengandung kedua nama dan fitur lainnya yang digunakan untuk mendefinisikan kemiripan
    k : tipe data integer (int)
        Banyaknya jumlah rekomendasi yang diberikan
    ---


    Pada index ini, kita mengambil k dengan nilai similarity terbesar
    pada index matrix yang diberikan (i).
    """


    # Mengambil data dengan menggunakan argpartition untuk melakukan partisi secara tidak langsung sepanjang sumbu yang diberikan
    # Dataframe diubah menjadi numpy
    # Range(start, stop, step)
    index = similarity_data.loc[:,nama].to_numpy().argpartition(
        range(-1, -k, -1))

    # Mengambil data dengan similarity terbesar dari index yang ada
    closest = similarity_data.columns[index[-1:-(k+2):-1]]

    # Drop nama_resto agar nama resto yang dicari tidak muncul dalam daftar rekomendasi
    closest = closest.drop(nama, errors='ignore')

    return pd.DataFrame(closest).merge(items).head(k)

"""Setelah itu, kita akan masukkan data yang akan kita cari rekomendasi nya."""

data[data.name.eq('Kebun Binatang Ragunan')]

"""Lalu kita akan mencari rekomendasi nya dengan fungsi yang telah kita buat sebelumnya."""

location_recommendations('Kebun Binatang Ragunan')

"""Terlihat bahwa fungsi merekomendasikan tempat wisata dengan category yang sama.

## Model Development dengan Collaborative Filtering
"""

# Import library
import pandas as pd
import numpy as np
from zipfile import ZipFile
import tensorflow as tf
from tensorflow import keras
from tensorflow.keras import layers
from pathlib import Path
import matplotlib.pyplot as plt

"""Kita akan membuat variabel df yang memuat dataset rating_df."""

df = rating_df
df

"""Setelah itu, kita melakukan persiapan data untuk menyandikan (encode) fitur User_Id ke dalam indeks integer."""

# Mengubah userID menjadi list tanpa nilai yang sama
user_ids = df['User_Id'].unique().tolist()
print('list userID: ', user_ids)

# Melakukan encoding userID
user_to_user_encoded = {x: i for i, x in enumerate(user_ids)}
print('encoded userID : ', user_to_user_encoded)

# Melakukan proses encoding angka ke ke userID
user_encoded_to_user = {i: x for i, x in enumerate(user_ids)}
print('encoded angka ke userID: ', user_encoded_to_user)

"""Setelah itu, kita melakukan persiapan data untuk menyandikan (encode) fitur Place_Id ke dalam indeks integer."""

# Mengubah placeID menjadi list tanpa nilai yang sama
place_ids = df['Place_Id'].unique().tolist()

# Melakukan proses encoding placeID
place_to_place_encoded = {x: i for i, x in enumerate(place_ids)}

# Melakukan proses encoding angka ke placeID
place_encoded_to_place = {i: x for i, x in enumerate(place_ids)}

"""Berikutnya, kita akan memetakan userID dan placeID ke dataframe yang berkaitan."""

# Mapping userID ke dataframe user
df['user'] = df['User_Id'].map(user_to_user_encoded)

# Mapping placeID ke dataframe place
df['place'] = df['Place_Id'].map(place_to_place_encoded)

"""Setelah itu, kita akan mengecek beberapa hal dalam data seperti jumlah user, jumlah tempat wisata, dan mengubah nilai rating menjadi float."""

# Mendapatkan jumlah user
num_users = len(user_to_user_encoded)
print(num_users)

# Mendapatkan jumlah place
num_place = len(place_encoded_to_place)
print(num_place)

# Mengubah rating menjadi nilai float
df['Place_Ratings'] = df['Place_Ratings'].values.astype(np.float32)

# Nilai minimum rating
min_rating = min(df['Place_Ratings'])

# Nilai maksimal rating
max_rating = max(df['Place_Ratings'])

print('Number of User: {}, Number of place: {}, Min Rating: {}, Max Rating: {}'.format(
    num_users, num_place, min_rating, max_rating
))

"""Setelah itu, kita akan acak datanya terlebih dahulu agar distribusinya menjadi random."""

df = df.sample(frac=1, random_state=42)
df

"""Karena pada kolom user tidak terjadi pengacakan, maka kita akan mengacak kolom user terlebih dahulu."""

df['user'] = np.random.permutation(df['user'].values)
df

"""Selanjutnya, kita bagi data train dan validasi dengan komposisi 80:20. Namun sebelumnya, kita perlu memetakan (mapping) data user dan place menjadi satu value terlebih dahulu. Lalu, buatlah rating dalam skala 0 sampai 1 agar mudah dalam melakukan proses training."""

# Membuat variabel x untuk mencocokkan data user dan resto menjadi satu value
x = df[['user', 'place']].values

# Membuat variabel y untuk membuat rating dari hasil
y = df['Place_Ratings'].apply(lambda x: (x - min_rating) / (max_rating - min_rating)).values

# Membagi menjadi 80% data train dan 20% data validasi
train_indices = int(0.8 * df.shape[0])
x_train, x_val, y_train, y_val = (
    x[:train_indices],
    x[train_indices:],
    y[:train_indices],
    y[train_indices:]
)

print(x, y)

"""Setelah itu, kita membuat class RecommenderNet dengan keras Model class."""

class RecommenderNet(tf.keras.Model):

  # Insialisasi fungsi
  def __init__(self, num_users, num_place, embedding_size, **kwargs):
    super(RecommenderNet, self).__init__(**kwargs)
    self.num_users = num_users
    self.num_place = num_place
    self.embedding_size = embedding_size
    self.user_embedding = layers.Embedding( # layer embedding user
        num_users,
        embedding_size,
        embeddings_initializer = 'he_normal',
        embeddings_regularizer = keras.regularizers.l2(1e-4)
    )
    self.user_bias = layers.Embedding(num_users, 1) # layer embedding user bias
    self.place_embedding = layers.Embedding( # layer embeddings place
        num_place,
        embedding_size,
        embeddings_initializer = 'he_normal',
        embeddings_regularizer = keras.regularizers.l2(1e-6)
    )
    self.place_bias = layers.Embedding(num_place, 1) # layer embedding place bias

    self.dropout = layers.Dropout(0.3)

  def call(self, inputs):
    user_vector = self.user_embedding(inputs[:,0]) # memanggil layer embedding 1
    user_vector = self.dropout(user_vector)
    user_bias = self.user_bias(inputs[:, 0]) # memanggil layer embedding 2
    place_vector = self.place_embedding(inputs[:, 1]) # memanggil layer embedding 3
    place_vector = self.dropout(place_vector)
    place_bias = self.place_bias(inputs[:, 1]) # memanggil layer embedding 4

    dot_user_place = tf.tensordot(user_vector, place_vector, 2)

    x = dot_user_place + user_bias + place_bias

    return tf.nn.sigmoid(x) # activation sigmoid

"""Setelah itu, kita akan melakukan proses compile terhadap model, dengan menggunakan fungsi loss binary crossentropy, optimizer adam, dan matriks RMSE."""

model = RecommenderNet(num_users, num_place, 20) # inisialisasi model

# model compile
model.compile(
    loss = tf.keras.losses.BinaryCrossentropy(),
    optimizer = keras.optimizers.Adam(learning_rate=0.001),
    metrics=[tf.keras.metrics.RootMeanSquaredError()]
)

"""Setelah itu, kita akan melakukan training data."""

# Memulai training

history = model.fit(
    x = x_train,
    y = y_train,
    batch_size = 8,
    epochs = 100,
    validation_data = (x_val, y_val)
)

"""Setelah itu, kita akan melakukan visualisasi proses training, kita plot metrik evaluasi dengan matplotlib."""

plt.plot(history.history['root_mean_squared_error'])
plt.plot(history.history['val_root_mean_squared_error'])
plt.title('model_metrics')
plt.ylabel('root_mean_squared_error')
plt.xlabel('epoch')
plt.legend(['train', 'test'], loc='upper left')
plt.show()

"""Sebelum melakukan prediksi sistem rekomendasi, kita akan membuat variabel place_not_visited terlebih dahulu  karena daftar place_not_visited inilah yang akan menjadi resto yang kita rekomendasikan."""

place_df = place_new
df = pd.read_csv('tourism_rating.csv')

# Mengambil sample user
user_id = df['User_Id'].sample(1).iloc[0]
place_visited_by_user = df[df['User_Id'] == user_id]

# Operator bitwise (~), bisa diketahui di sini https://docs.python.org/3/reference/expressions.html
place_not_visited = place_df[~place_df['id'].isin(place_visited_by_user['Place_Id'].values)]['id']
place_not_visited = list(
    set(place_not_visited)
    .intersection(set(place_to_place_encoded.keys()))
)

place_not_visited = [[place_to_place_encoded.get(x)] for x in place_not_visited]
user_encoder = user_to_user_encoded.get(user_id)
user_place_array = np.hstack(
    ([[user_encoder]] * len(place_not_visited), place_not_visited)
)

"""Selanjutnya, kita akan melakukan prediksi untuk memperoleh 10 rekomendasi tempat wisata."""

ratings = model.predict(user_place_array).flatten()

top_ratings_indices = ratings.argsort()[-10:][::-1]
recommended_place_ids = [
    place_encoded_to_place.get(place_not_visited[x][0]) for x in top_ratings_indices
]

print('Showing recommendations for users: {}'.format(user_id))
print('===' * 9)
print('place with high ratings from user')
print('----' * 8)

top_place_user = (
    place_visited_by_user.sort_values(
        by = 'Place_Ratings',
        ascending=False
    )
    .head(5)['Place_Ratings'].values
)

place_df_rows = place_df[place_df['id'].isin(top_place_user)]
for row in place_df_rows.itertuples():
    print(row.name, ':', row.category)

print('----' * 8)
print('Top 10 place recommendation')
print('----' * 8)

recommended_place = place_df[place_df['id'].isin(recommended_place_ids)]
for row in recommended_place.itertuples():
    print(row.name, ':', row.category)